<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Conquering Reality - from a Reinforcement Learning perspective | Michael Nath's site</title> <meta name="author" content="Michael D. Nath"> <meta name="description" content="How could a popular class of reinforcement learning agent teach us how to live life?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://michael-nath.github.io/blog/2022/conquer-reality-with-rl/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Michael Nath's site</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/interests/">interests</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <style type="text/css">h1,h2,h3{font-family:'JetBrains Mono',monospace}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Conquering Reality - from a Reinforcement Learning perspective</h1> <p class="post-meta">December 25, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <blockquote> <p>TLDR - Good beliefs lead to a good life. We ask ourselves: “what can we learn about living life from the shortcomings of a class of reinforcement learning agents?” We propose that living is effectively holding some beliefs fixed (assumptions) and behaving under them. We can behave like reinforcement learning agents and update our assumptions off observations, but that doesn’t work well with our preconceptions. Instead, our internal updates should involve repeatedly asking what-ifs about our assumed limitations, playing the theories out, and adopting them if they bear fruit.</p> </blockquote> <p><strong>Disclaimer</strong>: I understand that I am writing this as someone who is relatively privileged. Many people in this world, including my family, are entangled in responsibilities and circumstances that unfortunately limit them from self-actualizing. Therefore, this post is written to those who can be in an environment allowing them opportunity to find themselves. This is the case for many college students, and as an undergraduate myself, this article is a product of my observations of life so far and my journey with reinforcement learning.</p> <p>Reality is a strange thing. On one hand, we tend to think of it as this ebb-and-flow of causalities parametrized by an infinite number of variables. My decision to buy and eat a hot dog depends on many factors: the presence of hot dog vendors, the price set by them, the safety of their hot dogs, perhaps even the looks and greetings they give us! Each factor is again parametrized, perhaps by the economy, the quality of life in the neighborhood, the vendor’s personal life — we get the point. In short, we can think of reality as decided by others, and it may be too complex for us to ever have a grip of it.</p> <p>But that is about <em>the reality</em>. Let’s talk about our individual realities, which is what we care about anyways (for better or worse). The relationship between <em>the reality</em> and an individual‘s reality can be roughly formalized as:</p> \[Reality = \bigcap_{p \in People}Reality_p\] <p>To begin our discussion, let’s first consider the quintessential Thompson Sampling reinforcement learning agent, who operates — as is the case in many state-of-the-art systems — by sampling from a maintained posterior distribution of parameters. Upon this <em>realization</em> of parameters, the agent constructs an optimal policy and plays the game of interest under that policy. After each observation it receives while playing, the agent tweaks the distribution of parameters so that they better reflect the internal structure of the game, and by working towards perfect knowledge of the game, our agent hopefully converges to an optimal player.</p> <p>Already we can draw many parallels between how we act in our individual realities, and of that realized by our counterparts in the world of reinforcement learning. In our own realities, we maintain a set of variables of life that we hold fixed — more informally known as our <em>assumptions</em>. These assumptions are crucial to living efficiently. For instance, a person commuting to work in NYC makes assumptions about the line status of the subway system, the fare, the behavior of other passengers on the train, and many more. It is only upon these assumptions that we can live our lives — in other words operate off some policy. The 1/2/3 to Chambers St is broken? Let’s take the A to World Trade Ctr instead. Assuming other passenger may not wear a mask, let me pack one.</p> <p>So let’s suppose this is the case: every person maintains their unique set of assumptions of the world, they play the game of life under some policy parameterized by these assumptions, and then what? A Thompson Sampling agent would advice us to update our assumptions given the consequences, updates that would ideally disregard assumptions not worth making, and take on new ones that let us perform better in life. But here is the thing, how do we update? Thompson Sampling agents benefit from the power of Bayes Rule, so does the same hold true for us?</p> <p>Whereas a reinforcement learning agent takes its parameter updates as truth, often we have that our assumptions are so strongly rooted that even the most compelling observations are no match for our stubbornness. Our internal bayesian updates become biased for our preconceptions, and against what we simply don’t wish to become reality. This could explain why many of us feel stings and become uncomfortable from learning about current events, seeking refuge in modern entertainment — television, video games, etc. Our typical problem-solving is often constrained to how we <em>think</em> a problem should be solved, with our policy comprised of methods taught by our parents, teachers, etc. If a different, more complete solution is presented to us, sometimes we absorb it to better understand the domain of that problem. Sometimes, however, we latch on to our ideology.</p> <p>It’s safe to say we are a lot like reinforcement learning agents, after all they are inspired by us! But in many ways we are better than them, why? Because while they have to be obedient to their design, we don’t. Indeed, human achievement across the ages arise from the art of <em>pivoting</em>. For emperors, pivoting could mean restructuring the empire upon hearing bad news; for entrepreneurs pivoting could mean switching markets upon discovering a bad product-market fit for the current audience; for the majority of us, pivoting could just mean adopting a growth mindset and reconsidering the assumptions we constrain our growth with.</p> <p>State-of-the-art information seeking Thompson Sampling agents operate under a particular realization of parameters (i.e. a theory) in an \(\epsilon\)-greedy fashion. What this means is that morst of the time the agent operates under some generated policy; however at some random instance, the agent samples a new set of beliefs from the continually updated posterior, effectively producing a new theory to work with. We don’t have to work with a theory for such long or be subject to such randomness. We can leverage our brain’s power as an answering machine to ask ourselves: <em>what am I assuming should not be the case?</em> Some assume work and fun should not be together in a sentence; some assume internships are needed to have a productive summer. Some assume pineapple and pizza do not go well together!</p> <h3 id="what-am-i-assuming-should-not-be-the-case"><em>What am I assuming should not be the case?</em></h3> <p>Asking the “what-if” that would challenge these assumptions is only the first step. What comes next, akin to how an agent experiments with a theory, is to take the initiative and play out the what-if in the world. It could be that under this theory, you accumulate negative rewards, but it could also be that this theory strikes gold. For most theories the possibly harmful repercussions are often exaggerated, in part due to our stubbornness and tendency to remain in the comfort zone. If the theory does cause harm, pivot.</p> <p>This idea sounds like we are “hacking reality,” and in some ways challenging our assumptions so frequently may be un-natural, but now I hope we feel somewhat free. We can take any shortcoming we have, and at any moment, try to live in a what-if of our choosing. One indicator of a good reinforcement learning agents is how well they gain epistemic certainty — knowing what they do not know — of their environment. There’s much active research in how to get agents to figure out which parameters to prioritize learning (recalling our subway commute example, the line status could bear the most information on which trains to take), and generally such algorithms embodying this information seeking behavior comes at a cost of compute time. We’re able to just leverage our wonderful mind and ask it the what-ifs that could help bring forth the realities seen in the shows and fantasies we indulge ourselves in.</p> <p>So, what’s the takeaway? Reinforcement learning, among other qualities, can serve as a philosophy challenging the relationship we have with the assumptions we fix in our realities. We are effectively only as good of a player in the game of a life as the beliefs we have. To realize an enriching reality, we must frequently reconsider our beliefs against the wishes of our innate stubbornness. We begin by asking the what-ifs about our daily habits, then move towards our aspirations and goals. We should not have to settle with less; learn the parameters optimal to your life as fast as possible, and exploit.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Michael D. Nath. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>