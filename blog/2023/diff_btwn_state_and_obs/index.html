<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The difference between a state and an observation | Michael Nath's site</title> <meta name="author" content="Michael D. Nath"> <meta name="description" content="How can a heated game of tic-tac-toe elucidate one of the most crucial distinctions in reinforcement learning?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://michael-nath.github.io/blog/2023/diff_btwn_state_and_obs/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Michael Nath's site</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/interests/">interests</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <style type="text/css">h1,h2,h3,h4{font-family:'JetBrains Mono',monospace}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">The difference between a state and an observation</h1> <p class="post-meta">March 4, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <blockquote> <p>When I first started learning reinforcement learning - as an intern conducting research at Brookhavens National Laboratories - there was one concept I just couldn’t grapple with: the <code class="language-plaintext highlighter-rouge">state</code>. Not only was it strange that we could just distill the environment in a vector (sometimes a scalar!), but I kept confusing <code class="language-plaintext highlighter-rouge">state</code> with its cousin, the <code class="language-plaintext highlighter-rouge">observation</code>. Sometimes I heard the concepts be used interchangably, other times the scientist would make certain to keep the two separate. While many good formal explanations of the differences <a href="https://ai.stackexchange.com/questions/5970/what-is-the-difference-between-an-observation-and-a-state-in-reinforcement-learn" rel="external nofollow noopener" target="_blank">exist</a>, I hope the following presents a more beginner-friendly and thought-provoking illustration of perhaps the most crucial distinction in reinforcement learning.</p> </blockquote> <h3 id="case-study-1-online-tic-tac-toe">Case Study #1: Online Tic-Tac-Toe</h3> <p>I won’t go into the spiel of rigorously defining what a Markov Decision Process (MDP) is, but I will leverage the <em>spirit</em> of a MDP as the basis for the following thought-experiment. Imagine you are trying to play a game of ultimate tic-tac-toe with your rival on the internet. You two mark your Xs and Os for a while, but all of a sudden you have to use the restroom! Of course, you deeply wish to best your rival in ultimate tic-tac-toe, but your body unfortunately has other plans. So you rush to the toilet, but along the way you spot your sibling and - preferring someone playing on your behalf than an automatic forfeit from being away from the keyboard - you command them to continue the game against your rival. While you’re on the toilet, your sibling sees the grid, thinks for a bit, and begins playing. You get out of the bathroom as soon as possbile, and kick your sibling off the keyboard. You see the new grid, sigh at their suboptimal plays, and continue the showdown.</p> <h4 id="state-as-a-mdp-formalism-described-informally"> <code class="language-plaintext highlighter-rouge">state</code> (as a MDP formalism) described informally</h4> <p>Now ask yourself: how was your sibling able to just play the game upon seeing the grid thus far, and how were you able to just play seeing the final result of your sibling’s plays? That’s because — and this is the main takeaway of a MDP — what really mattered was the grid’s configuration (a.k.a. its <code class="language-plaintext highlighter-rouge">state</code>) just before you made your next move. You don’t care how the board got to the way it looks now; the fact of the matter is that this <strong>is</strong> the configuration, and now you plan your next action.</p> <h4 id="observation-as-a-rl-formalism-described-informally"> <code class="language-plaintext highlighter-rouge">observation</code> (as a RL formalism) described informally</h4> <p>Simply put, an observation is what you <strong>perceive</strong> after you take some action in the environment you are in. In our tic-tac-toe example, you input your move into the tic-tac-toe program, it updates the grid, and after some time the grid is updated with your opponent’s response. You, as the player, could care less about how the program is orchestrating these inputs behind the scene. What matters to you is that the board displays your moves, and more importantly your rival’s moves. The idea of an <code class="language-plaintext highlighter-rouge">observation</code> may feel natural and intuitive, and that’s becaues it <em>is</em>. Observations are ubiquitous in our world: When it rains outside (as it is while I’m writing this), you have no idea what the molecular composition of the precipitative clouds are — all you perceive is the rain.</p> <h4 id="edge-case-observation--state-huh">Edge case: <code class="language-plaintext highlighter-rouge">observation</code> \(=\) <code class="language-plaintext highlighter-rouge">state</code> (huh?)</h4> <p>Ok, I should address an elephant in the room. In the tic-tac-toe example, you might have noticed that your <code class="language-plaintext highlighter-rouge">observation</code> of the grid corresponds to the <code class="language-plaintext highlighter-rouge">state</code> of the grid <strong>exactly</strong>. The configuration of the grid is exactly the locations of Xs and Os and any unmarked spots, and that’s precisely what you see on the computer screen as well! Indeed, it is possible for your <code class="language-plaintext highlighter-rouge">observation</code> of the environment to be exactly its internal <code class="language-plaintext highlighter-rouge">state</code>, and this is what has deceived me for so long. But don’t let this deceive you as well — this is merely an exception, not the rule. In many structured, deterministic settings such as board games, this is the case. But we should not expect most environments to adhere to this rare equality, and in fact, the ones worth tackling shatter this equality.</p> <h3 id="case-study-2-glitched-tic-tac-toe">Case Study #2: Glitched Tic-Tac-Toe</h3> <p>I hope the next thought experiment illuminates the distinction between <code class="language-plaintext highlighter-rouge">state</code> and <code class="language-plaintext highlighter-rouge">observation</code>, and more importanly how much more fun solving the environment becomes when <code class="language-plaintext highlighter-rouge">state</code> \(\neq\) <code class="language-plaintext highlighter-rouge">observation</code>. Imagine that while in the middle of competing against your rival in the same game of ultimate tic-tac-toe, your sibling messes with the display cables behind your monitor. As a result, the screen starts to glitch in such a way that some of the squares on the grid are covered by static and/or obscuring colors. You want to get your sibling away from the monitor, but the match is heated, and there’s such little time given for each player’s turn! Of course you don’t wish to forfeit, so now you just have to deal with what little you <strong>observe</strong>, and play to the best of your abilities.</p> <h4 id="partial-observability-observation-neq-state">Partial Observability (<code class="language-plaintext highlighter-rouge">observation</code> \(\neq\) <code class="language-plaintext highlighter-rouge">state</code>)</h4> <blockquote> <p>Life is a series of POMDPs - Dr. Mykel Kochenderfer</p> </blockquote> <p>By mucking with the display cables, your sibling has thrown you into what’s known as a Partially Observable Markov Decision Process (POMDP). While the name may sound scary, the essence of a POMPDP is that now your <code class="language-plaintext highlighter-rouge">observation</code> cannot fully capture the <code class="language-plaintext highlighter-rouge">state</code> of the environment. The state remains untouched — the board’s internal configuration is the same, the letters are still mapped to the same spots in the code — but the glitches on the screen prevent you from knowing what the board completely looks like at any given moment. This is the <em>partially observable</em> part of the MDP. I hope it’s clear how the <code class="language-plaintext highlighter-rouge">observation</code> is no longer equal to the <code class="language-plaintext highlighter-rouge">state</code>, and I also invite you to think of how you may try to beat your rival in this glitched game of tic-tac-toe. Perhaps the stochasticity of the glitches compel you to jot down the marked locations while you can still see them (in reinforcement learning agents, this could be a <a href="https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial" rel="external nofollow noopener" target="_blank">replay buffer</a>). In essence, so many more strategies (more formally, policies) to play this game can now exist because your <code class="language-plaintext highlighter-rouge">observations</code> don’t explain the state, and thus the complexity of the game greatly increases. Most importantly, the game becomes more interesting to play.</p> <h4 id="working-with-observations-introducing-agent-states">Working with observations: introducing <code class="language-plaintext highlighter-rouge">agent states</code> </h4> <p>Besides being a great tic-tac-toe player, you’re also a great hacker, and so you think about somehow forcing the tic-tac-toe program to significantly lengthen the duration of a turn. This would allow you to fully capture the grid configuration (assuming the glitches covering a particular square migrate to another spot)! But this would be unlikely (and considered cheating!), and in general, it is <strong>incredibly</strong> difficult to change with the environment’s <code class="language-plaintext highlighter-rouge">state</code> to accomodate for your observability. So, you have to work with what you’ve got, and you consider bringing on <code class="language-plaintext highlighter-rouge">agent states</code>. Informally, <code class="language-plaintext highlighter-rouge">agent states</code> are constructed by the agent to best encapsulate the environment based on what they’ve observed thus far. In the normal tic-tac-toe example you may construct your agent state to mimic that of the environment (<code class="language-plaintext highlighter-rouge">agent states</code> \(=\) <code class="language-plaintext highlighter-rouge">state</code>), but in general this is not likely. In some ways, <code class="language-plaintext highlighter-rouge">agent states</code> offer less than what knowing the environment <code class="language-plaintext highlighter-rouge">state</code> could offer you, but in many ways <code class="language-plaintext highlighter-rouge">agent states</code> could offer so much more. You may choose your <code class="language-plaintext highlighter-rouge">agent state</code> to include information such as the locations of not only the marked and open spots, but also the glitched spots. You can include the number of ratio of your Xs to your rival’s Os, which could be utilized in some clever strategy. For those who are interested in a mathematical description, you can think of <code class="language-plaintext highlighter-rouge">agent states</code> as the result of the following:</p> \[(X_1, X_2, X_3, \dots X_K) = f((O_1, O_2, \dots, O_N))\] <p>In other words, \(f\) is some agent-designed transformation on its \(N\) <code class="language-plaintext highlighter-rouge">observations</code> to generate a state \(X\) comprised of \(K\) components. Going along with the possible choices of states, some of the \(X\)s may correspond to grid locations of glitched squares, for instance. Much work in modern reinforcement learning goes into deriving such optimal \(f\) that the agent can leverage, where some methods figure out the components of the <code class="language-plaintext highlighter-rouge">agent state</code> for you (e.g. deep reinforcement learning)! Lastly, note that the choice of \(f\) is crucial for your performance. It is indeed possible to construct a poor \(f\) that yields <code class="language-plaintext highlighter-rouge">agent states</code> that actually sabotage your performance, where you’d have been better off working with just the <code class="language-plaintext highlighter-rouge">observations</code>!</p> <h4 id="observations-and-states-in-our-life"> <code class="language-plaintext highlighter-rouge">observations</code> and <code class="language-plaintext highlighter-rouge">states</code> in our life</h4> <p>We love to make order out of chaos. But sometimes there is no chaos in our daily life, and so the inherent structure of the environment is enough for us to live by. For instance, a typical 9-to-5 senior employee — as implied by the term “9-to-5” — does not have to do much thinking other than what is pertinent to their job or personal life. However, a budding teenager has their whole life ahead of them! The environments they inhabit are rapidly changing, their observations of the world may have no bearings on what is actually true. Their raw observations could easily deceive them, so what might they do? They start to build out a perspective — internal structures similar to <code class="language-plaintext highlighter-rouge">agent states</code> — that help them process their observations and help them act in a way that accomplishes their goals. This transformation from observations to worldview is what we called our mindset, and of course, mindsets can significantly differ. Your \(f\) may be different from my \(f\), and if we have vastly different goals in life, your <code class="language-plaintext highlighter-rouge">agent states</code> could serve you well but completely sabotage my life. What’s important here though, is that \(f\) can be changed, with some effort of course. So act in your environment, updating its <code class="language-plaintext highlighter-rouge">state</code>, gather some <code class="language-plaintext highlighter-rouge">observations</code>, and build out a wicked \(f\).</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Michael D. Nath. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>