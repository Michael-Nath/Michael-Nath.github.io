<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://michael-nath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://michael-nath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T06:54:33+00:00</updated><id>https://michael-nath.github.io/feed.xml</id><title type="html">Michael Nath’s site</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The difference between a state and an observation</title><link href="https://michael-nath.github.io/blog/2023/diff_btwn_state_and_obs/" rel="alternate" type="text/html" title="The difference between a state and an observation"/><published>2023-03-04T00:00:00+00:00</published><updated>2023-03-04T00:00:00+00:00</updated><id>https://michael-nath.github.io/blog/2023/diff_btwn_state_and_obs</id><content type="html" xml:base="https://michael-nath.github.io/blog/2023/diff_btwn_state_and_obs/"><![CDATA[<blockquote> <p>When I first started learning reinforcement learning - as an intern conducting research at Brookhavens National Laboratories - there was one concept I just couldn’t grapple with: the <code class="language-plaintext highlighter-rouge">state</code>. Not only was it strange that we could just distill the environment in a vector (sometimes a scalar!), but I kept confusing <code class="language-plaintext highlighter-rouge">state</code> with its cousin, the <code class="language-plaintext highlighter-rouge">observation</code>. Sometimes I heard the concepts be used interchangably, other times the scientist would make certain to keep the two separate. While many good formal explanations of the differences <a href="https://ai.stackexchange.com/questions/5970/what-is-the-difference-between-an-observation-and-a-state-in-reinforcement-learn">exist</a>, I hope the following presents a more beginner-friendly and thought-provoking illustration of perhaps the most crucial distinction in reinforcement learning.</p> </blockquote> <h3 id="case-study-1-online-tic-tac-toe">Case Study #1: Online Tic-Tac-Toe</h3> <p>I won’t go into the spiel of rigorously defining what a Markov Decision Process (MDP) is, but I will leverage the <em>spirit</em> of a MDP as the basis for the following thought-experiment. Imagine you are trying to play a game of ultimate tic-tac-toe with your rival on the internet. You two mark your Xs and Os for a while, but all of a sudden you have to use the restroom! Of course, you deeply wish to best your rival in ultimate tic-tac-toe, but your body unfortunately has other plans. So you rush to the toilet, but along the way you spot your sibling and - preferring someone playing on your behalf than an automatic forfeit from being away from the keyboard - you command them to continue the game against your rival. While you’re on the toilet, your sibling sees the grid, thinks for a bit, and begins playing. You get out of the bathroom as soon as possbile, and kick your sibling off the keyboard. You see the new grid, sigh at their suboptimal plays, and continue the showdown.</p> <h4 id="state-as-a-mdp-formalism-described-informally"><code class="language-plaintext highlighter-rouge">state</code> (as a MDP formalism) described informally</h4> <p>Now ask yourself: how was your sibling able to just play the game upon seeing the grid thus far, and how were you able to just play seeing the final result of your sibling’s plays? That’s because — and this is the main takeaway of a MDP — what really mattered was the grid’s configuration (a.k.a. its <code class="language-plaintext highlighter-rouge">state</code>) just before you made your next move. You don’t care how the board got to the way it looks now; the fact of the matter is that this <strong>is</strong> the configuration, and now you plan your next action.</p> <h4 id="observation-as-a-rl-formalism-described-informally"><code class="language-plaintext highlighter-rouge">observation</code> (as a RL formalism) described informally</h4> <p>Simply put, an observation is what you <strong>perceive</strong> after you take some action in the environment you are in. In our tic-tac-toe example, you input your move into the tic-tac-toe program, it updates the grid, and after some time the grid is updated with your opponent’s response. You, as the player, could care less about how the program is orchestrating these inputs behind the scene. What matters to you is that the board displays your moves, and more importantly your rival’s moves. The idea of an <code class="language-plaintext highlighter-rouge">observation</code> may feel natural and intuitive, and that’s becaues it <em>is</em>. Observations are ubiquitous in our world: When it rains outside (as it is while I’m writing this), you have no idea what the molecular composition of the precipitative clouds are — all you perceive is the rain.</p> <h4 id="edge-case-observation--state-huh">Edge case: <code class="language-plaintext highlighter-rouge">observation</code> \(=\) <code class="language-plaintext highlighter-rouge">state</code> (huh?)</h4> <p>Ok, I should address an elephant in the room. In the tic-tac-toe example, you might have noticed that your <code class="language-plaintext highlighter-rouge">observation</code> of the grid corresponds to the <code class="language-plaintext highlighter-rouge">state</code> of the grid <strong>exactly</strong>. The configuration of the grid is exactly the locations of Xs and Os and any unmarked spots, and that’s precisely what you see on the computer screen as well! Indeed, it is possible for your <code class="language-plaintext highlighter-rouge">observation</code> of the environment to be exactly its internal <code class="language-plaintext highlighter-rouge">state</code>, and this is what has deceived me for so long. But don’t let this deceive you as well — this is merely an exception, not the rule. In many structured, deterministic settings such as board games, this is the case. But we should not expect most environments to adhere to this rare equality, and in fact, the ones worth tackling shatter this equality.</p> <h3 id="case-study-2-glitched-tic-tac-toe">Case Study #2: Glitched Tic-Tac-Toe</h3> <p>I hope the next thought experiment illuminates the distinction between <code class="language-plaintext highlighter-rouge">state</code> and <code class="language-plaintext highlighter-rouge">observation</code>, and more importanly how much more fun solving the environment becomes when <code class="language-plaintext highlighter-rouge">state</code> \(\neq\) <code class="language-plaintext highlighter-rouge">observation</code>. Imagine that while in the middle of competing against your rival in the same game of ultimate tic-tac-toe, your sibling messes with the display cables behind your monitor. As a result, the screen starts to glitch in such a way that some of the squares on the grid are covered by static and/or obscuring colors. You want to get your sibling away from the monitor, but the match is heated, and there’s such little time given for each player’s turn! Of course you don’t wish to forfeit, so now you just have to deal with what little you <strong>observe</strong>, and play to the best of your abilities.</p> <h4 id="partial-observability-observation-neq-state">Partial Observability (<code class="language-plaintext highlighter-rouge">observation</code> \(\neq\) <code class="language-plaintext highlighter-rouge">state</code>)</h4> <blockquote> <p>Life is a series of POMDPs - Dr. Mykel Kochenderfer</p> </blockquote> <p>By mucking with the display cables, your sibling has thrown you into what’s known as a Partially Observable Markov Decision Process (POMDP). While the name may sound scary, the essence of a POMPDP is that now your <code class="language-plaintext highlighter-rouge">observation</code> cannot fully capture the <code class="language-plaintext highlighter-rouge">state</code> of the environment. The state remains untouched — the board’s internal configuration is the same, the letters are still mapped to the same spots in the code — but the glitches on the screen prevent you from knowing what the board completely looks like at any given moment. This is the <em>partially observable</em> part of the MDP. I hope it’s clear how the <code class="language-plaintext highlighter-rouge">observation</code> is no longer equal to the <code class="language-plaintext highlighter-rouge">state</code>, and I also invite you to think of how you may try to beat your rival in this glitched game of tic-tac-toe. Perhaps the stochasticity of the glitches compel you to jot down the marked locations while you can still see them (in reinforcement learning agents, this could be a <a href="https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial">replay buffer</a>). In essence, so many more strategies (more formally, policies) to play this game can now exist because your <code class="language-plaintext highlighter-rouge">observations</code> don’t explain the state, and thus the complexity of the game greatly increases. Most importantly, the game becomes more interesting to play.</p> <h4 id="working-with-observations-introducing-agent-states">Working with observations: introducing <code class="language-plaintext highlighter-rouge">agent states</code></h4> <p>Besides being a great tic-tac-toe player, you’re also a great hacker, and so you think about somehow forcing the tic-tac-toe program to significantly lengthen the duration of a turn. This would allow you to fully capture the grid configuration (assuming the glitches covering a particular square migrate to another spot)! But this would be unlikely (and considered cheating!), and in general, it is <strong>incredibly</strong> difficult to change with the environment’s <code class="language-plaintext highlighter-rouge">state</code> to accomodate for your observability. So, you have to work with what you’ve got, and you consider bringing on <code class="language-plaintext highlighter-rouge">agent states</code>. Informally, <code class="language-plaintext highlighter-rouge">agent states</code> are constructed by the agent to best encapsulate the environment based on what they’ve observed thus far. In the normal tic-tac-toe example you may construct your agent state to mimic that of the environment (<code class="language-plaintext highlighter-rouge">agent states</code> \(=\) <code class="language-plaintext highlighter-rouge">state</code>), but in general this is not likely. In some ways, <code class="language-plaintext highlighter-rouge">agent states</code> offer less than what knowing the environment <code class="language-plaintext highlighter-rouge">state</code> could offer you, but in many ways <code class="language-plaintext highlighter-rouge">agent states</code> could offer so much more. You may choose your <code class="language-plaintext highlighter-rouge">agent state</code> to include information such as the locations of not only the marked and open spots, but also the glitched spots. You can include the number of ratio of your Xs to your rival’s Os, which could be utilized in some clever strategy. For those who are interested in a mathematical description, you can think of <code class="language-plaintext highlighter-rouge">agent states</code> as the result of the following:</p> \[(X_1, X_2, X_3, \dots X_K) = f((O_1, O_2, \dots, O_N))\] <p>In other words, \(f\) is some agent-designed transformation on its \(N\) <code class="language-plaintext highlighter-rouge">observations</code> to generate a state \(X\) comprised of \(K\) components. Going along with the possible choices of states, some of the \(X\)s may correspond to grid locations of glitched squares, for instance. Much work in modern reinforcement learning goes into deriving such optimal \(f\) that the agent can leverage, where some methods figure out the components of the <code class="language-plaintext highlighter-rouge">agent state</code> for you (e.g. deep reinforcement learning)! Lastly, note that the choice of \(f\) is crucial for your performance. It is indeed possible to construct a poor \(f\) that yields <code class="language-plaintext highlighter-rouge">agent states</code> that actually sabotage your performance, where you’d have been better off working with just the <code class="language-plaintext highlighter-rouge">observations</code>!</p> <h4 id="observations-and-states-in-our-life"><code class="language-plaintext highlighter-rouge">observations</code> and <code class="language-plaintext highlighter-rouge">states</code> in our life</h4> <p>We love to make order out of chaos. But sometimes there is no chaos in our daily life, and so the inherent structure of the environment is enough for us to live by. For instance, a typical 9-to-5 senior employee — as implied by the term “9-to-5” — does not have to do much thinking other than what is pertinent to their job or personal life. However, a budding teenager has their whole life ahead of them! The environments they inhabit are rapidly changing, their observations of the world may have no bearings on what is actually true. Their raw observations could easily deceive them, so what might they do? They start to build out a perspective — internal structures similar to <code class="language-plaintext highlighter-rouge">agent states</code> — that help them process their observations and help them act in a way that accomplishes their goals. This transformation from observations to worldview is what we called our mindset, and of course, mindsets can significantly differ. Your \(f\) may be different from my \(f\), and if we have vastly different goals in life, your <code class="language-plaintext highlighter-rouge">agent states</code> could serve you well but completely sabotage my life. What’s important here though, is that \(f\) can be changed, with some effort of course. So act in your environment, updating its <code class="language-plaintext highlighter-rouge">state</code>, gather some <code class="language-plaintext highlighter-rouge">observations</code>, and build out a wicked \(f\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How can a heated game of tic-tac-toe elucidate one of the most crucial distinctions in reinforcement learning?]]></summary></entry><entry><title type="html">Conquering Reality - from a Reinforcement Learning perspective</title><link href="https://michael-nath.github.io/blog/2022/conquer-reality-with-rl/" rel="alternate" type="text/html" title="Conquering Reality - from a Reinforcement Learning perspective"/><published>2022-12-25T00:00:00+00:00</published><updated>2022-12-25T00:00:00+00:00</updated><id>https://michael-nath.github.io/blog/2022/conquer-reality-with-rl</id><content type="html" xml:base="https://michael-nath.github.io/blog/2022/conquer-reality-with-rl/"><![CDATA[<blockquote> <p>TLDR - Good beliefs lead to a good life. We ask ourselves: “what can we learn about living life from the shortcomings of a class of reinforcement learning agents?” We propose that living is effectively holding some beliefs fixed (assumptions) and behaving under them. We can behave like reinforcement learning agents and update our assumptions off observations, but that doesn’t work well with our preconceptions. Instead, our internal updates should involve repeatedly asking what-ifs about our assumed limitations, playing the theories out, and adopting them if they bear fruit.</p> </blockquote> <p><strong>Disclaimer</strong>: I understand that I am writing this as someone who is relatively privileged. Many people in this world, including my family, are entangled in responsibilities and circumstances that unfortunately limit them from self-actualizing. Therefore, this post is written to those who can be in an environment allowing them opportunity to find themselves. This is the case for many college students, and as an undergraduate myself, this article is a product of my observations of life so far and my journey with reinforcement learning.</p> <p>Reality is a strange thing. On one hand, we tend to think of it as this ebb-and-flow of causalities parametrized by an infinite number of variables. My decision to buy and eat a hot dog depends on many factors: the presence of hot dog vendors, the price set by them, the safety of their hot dogs, perhaps even the looks and greetings they give us! Each factor is again parametrized, perhaps by the economy, the quality of life in the neighborhood, the vendor’s personal life — we get the point. In short, we can think of reality as decided by others, and it may be too complex for us to ever have a grip of it.</p> <p>But that is about <em>the reality</em>. Let’s talk about our individual realities, which is what we care about anyways (for better or worse). The relationship between <em>the reality</em> and an individual‘s reality can be roughly formalized as:</p> \[Reality = \bigcap_{p \in People}Reality_p\] <p>To begin our discussion, let’s first consider the quintessential Thompson Sampling reinforcement learning agent, who operates — as is the case in many state-of-the-art systems — by sampling from a maintained posterior distribution of parameters. Upon this <em>realization</em> of parameters, the agent constructs an optimal policy and plays the game of interest under that policy. After each observation it receives while playing, the agent tweaks the distribution of parameters so that they better reflect the internal structure of the game, and by working towards perfect knowledge of the game, our agent hopefully converges to an optimal player.</p> <p>Already we can draw many parallels between how we act in our individual realities, and of that realized by our counterparts in the world of reinforcement learning. In our own realities, we maintain a set of variables of life that we hold fixed — more informally known as our <em>assumptions</em>. These assumptions are crucial to living efficiently. For instance, a person commuting to work in NYC makes assumptions about the line status of the subway system, the fare, the behavior of other passengers on the train, and many more. It is only upon these assumptions that we can live our lives — in other words operate off some policy. The 1/2/3 to Chambers St is broken? Let’s take the A to World Trade Ctr instead. Assuming other passenger may not wear a mask, let me pack one.</p> <p>So let’s suppose this is the case: every person maintains their unique set of assumptions of the world, they play the game of life under some policy parameterized by these assumptions, and then what? A Thompson Sampling agent would advice us to update our assumptions given the consequences, updates that would ideally disregard assumptions not worth making, and take on new ones that let us perform better in life. But here is the thing, how do we update? Thompson Sampling agents benefit from the power of Bayes Rule, so does the same hold true for us?</p> <p>Whereas a reinforcement learning agent takes its parameter updates as truth, often we have that our assumptions are so strongly rooted that even the most compelling observations are no match for our stubbornness. Our internal bayesian updates become biased for our preconceptions, and against what we simply don’t wish to become reality. This could explain why many of us feel stings and become uncomfortable from learning about current events, seeking refuge in modern entertainment — television, video games, etc. Our typical problem-solving is often constrained to how we <em>think</em> a problem should be solved, with our policy comprised of methods taught by our parents, teachers, etc. If a different, more complete solution is presented to us, sometimes we absorb it to better understand the domain of that problem. Sometimes, however, we latch on to our ideology.</p> <p>It’s safe to say we are a lot like reinforcement learning agents, after all they are inspired by us! But in many ways we are better than them, why? Because while they have to be obedient to their design, we don’t. Indeed, human achievement across the ages arise from the art of <em>pivoting</em>. For emperors, pivoting could mean restructuring the empire upon hearing bad news; for entrepreneurs pivoting could mean switching markets upon discovering a bad product-market fit for the current audience; for the majority of us, pivoting could just mean adopting a growth mindset and reconsidering the assumptions we constrain our growth with.</p> <p>State-of-the-art information seeking Thompson Sampling agents operate under a particular realization of parameters (i.e. a theory) in an \(\epsilon\)-greedy fashion. What this means is that morst of the time the agent operates under some generated policy; however at some random instance, the agent samples a new set of beliefs from the continually updated posterior, effectively producing a new theory to work with. We don’t have to work with a theory for such long or be subject to such randomness. We can leverage our brain’s power as an answering machine to ask ourselves: <em>what am I assuming should not be the case?</em> Some assume work and fun should not be together in a sentence; some assume internships are needed to have a productive summer. Some assume pineapple and pizza do not go well together!</p> <h3 id="what-am-i-assuming-should-not-be-the-case"><em>What am I assuming should not be the case?</em></h3> <p>Asking the “what-if” that would challenge these assumptions is only the first step. What comes next, akin to how an agent experiments with a theory, is to take the initiative and play out the what-if in the world. It could be that under this theory, you accumulate negative rewards, but it could also be that this theory strikes gold. For most theories the possibly harmful repercussions are often exaggerated, in part due to our stubbornness and tendency to remain in the comfort zone. If the theory does cause harm, pivot.</p> <p>This idea sounds like we are “hacking reality,” and in some ways challenging our assumptions so frequently may be un-natural, but now I hope we feel somewhat free. We can take any shortcoming we have, and at any moment, try to live in a what-if of our choosing. One indicator of a good reinforcement learning agents is how well they gain epistemic certainty — knowing what they do not know — of their environment. There’s much active research in how to get agents to figure out which parameters to prioritize learning (recalling our subway commute example, the line status could bear the most information on which trains to take), and generally such algorithms embodying this information seeking behavior comes at a cost of compute time. We’re able to just leverage our wonderful mind and ask it the what-ifs that could help bring forth the realities seen in the shows and fantasies we indulge ourselves in.</p> <p>So, what’s the takeaway? Reinforcement learning, among other qualities, can serve as a philosophy challenging the relationship we have with the assumptions we fix in our realities. We are effectively only as good of a player in the game of a life as the beliefs we have. To realize an enriching reality, we must frequently reconsider our beliefs against the wishes of our innate stubbornness. We begin by asking the what-ifs about our daily habits, then move towards our aspirations and goals. We should not have to settle with less; learn the parameters optimal to your life as fast as possible, and exploit.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How could a popular class of reinforcement learning agent teach us how to live life?]]></summary></entry></feed>